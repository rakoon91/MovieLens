	
title: "HarvardX PH125.9x Data Science Capstone"
	author: "Rakan Bassas"
	date: "23 June 2020"
	output: pdf_document
	---
	

	```{r, include=FALSE}
	knitr::opts_chunk$set(echo = TRUE)
	```
	

	# 1. Executive Summary
	

	### Background and Motivation
	A recommender framework or a suggestion framework is a subclass of data sifting framework that looks to foresee the "rating" or "inclination" a client would provide for a thing. In this task the things are films. 
	
	Recommender frameworks are used in an assortment of regions including films, music, news, books, research articles, search inquiries, social labels, and items all in all. There are likewise recommender frameworks for specialists teammates, jokes, caf√©s, articles of clothing, monetary administrations, life coverage, sentimental accomplices (web based dating), and Twitter page. Significant organizations, for example, Amazon, Netflix and Spotify use proposal frameworks. A solid proposal framework was of such significance that in 2006, Netflix offered a million dollar prize to any individual who could improve the viability of its suggestion framework by 10%. 
	
	It ought to be noticed that the triumphant Netflix model used a troupe of extremely complex models, and the group went through a while idealizing the gathering. While they won the main prize, no notice is made that can be openly found concerning the degree of prescient precision, as their objective was not to anticipate appraisals yet just prescribe films liable to be delighted in by a client. Therefore the Netflix issue and our own test is a lot of various in its objectives.
	A recommender framework or a suggestion framework is a subclass of data sifting framework that looks to foresee the "rating" or "inclination" a client would provide for a thing. In this task the things are films. 
	
	

	### DataSet
	For this project a movie rating predictor is created using the 'MovieLens' dataset. This data set can be found and downloaded here:
	

	- [MovieLens 10M dataset] https://grouplens.org/datasets/movielens/10m/
	- [MovieLens 10M dataset - zip file] http://files.grouplens.org/datasets/movielens/ml-10m.zip
	

	### Goal
	The objective is to prepare an AI calculation utilizing the contributions of a gave preparing subset to foresee film evaluations in an approval set. 
	
	The emphasis is on the prescient exactness of the calculation, which is interestingly with past Kaggle rivalries where RMSE or MAE were utilized as benchmarks. During investigation we will audit RMSE as a guide, while utilizing unadulterated precision as the unequivocal factor on whether to continue with a calculation or not. We will at long last report both RMSE and Exactness.
	

	### Data Loading and Setup
	We will use and burden a few bundles from CRAN to help with our investigation. These will be consequently downloaded and introduced during code execution. According to the venture rules, the dataset will initially be part into a preparation and approval set (10%), and the preparation set will at that point be additionally part into a train/test set with the test set being 10% of the preparation set.


	

	```{r, include=FALSE}
	if(!require(readr)) install.packages("readr")
	if(!require(dplyr)) install.packages("dplyr")
	if(!require(tidyr)) install.packages("tidyr")
	if(!require(stringr)) install.packages("stringr")
	if(!require(ggplot2)) install.packages("ggplot2")
	if(!require(gridExtra)) install.packages("gridExtra")
	if(!require(dslabs)) install.packages("dslabs")
	if(!require(data.table)) install.packages("data.table")
	if(!require(ggrepel)) install.packages("ggrepel")
	if(!require(ggthemes)) install.packages("ggthemes")
	
	library(readr)
	library(dplyr)
	library(tidyr)
	library(stringr)
	library(ggplot2)
	library(gridExtra)
	library(dslabs)
	library(data.table) # for fread if required
	library(ggrepel) # for some plots
	library(ggthemes) # cool ggplot themes, check it out
	if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
	if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
	
	dl <- tempfile()
	download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)
	
	ratings <- read.table(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
	                      col.names = c("userId", "movieId", "rating", "timestamp"))
	movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
	
	# If your read.table fails, remove it and replace with the fread line below from the data.table package after unzipping
	#ratings <- fread(text = gsub("::", "\t", readLines("ml-10M100K/ratings.dat")), data.table=TRUE, col.names = c("userId", "movieId", "rating", "timestamp"))
	#movies <- str_split_fixed(readLines("ml-10M100K/movies.dat"), "\\::", 3)
	
	
	colnames(movies) <- c("movieId", "title", "genres")
	movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
	                                           title = as.character(title),
	                                           genres = as.character(genres))
	
	movielens <- left_join(ratings, movies, by = "movieId")
	
	set.seed(1)
	test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
	edx <- movielens[-test_index,]
	temp <- movielens[test_index,]
	
	validation <- temp %>% 
	  semi_join(edx, by = "movieId") %>%
	  semi_join(edx, by = "userId")
	
	removed <- anti_join(temp, validation)
	edx <- rbind(edx, removed)
	```
	

	# 2. Methods and Analysis
	## Exploratory Analysis
	

	```{r, echo=FALSE}
	rm(removed, test_index)
	```
	

	A survey of the edx dataset shows 6 sections. The timestamp should be changed over whenever utilized, and discharge year should be part from the title if to be utilized for expectation. Kinds is a solitary channel delimited string containing the different sort classes a film may be classified under, and this should be part out on the off chance that it influences rating result.
	
	

	```{r, include=TRUE, echo=FALSE}
	head(edx)
	```
	

	

	There are no missing qualities. How about we survey a rundown of the dataset.
	

	```{r, include=FALSE, echo=FALSE}
	anyNA(edx)
	```
	

	```{r, include=TRUE, echo=FALSE}
	summary(edx)
	```
	

	```{r, include=FALSE, echo=FALSE}
	edx %>% summarize(n_users = n_distinct(userId), n_movies = n_distinct(movieId))
	```
	Our dataset contains ~70000 remarkable clients offering appraisals to ~ 10700 distinct motion pictures
	

	```{r, include=FALSE, echo=FALSE}
	unique(edx$rating)
	```
	There are 10 distinctive rating scores, most minimal is 0.5 and most noteworthy is 5.
	

	\pagebreak
	How about we investigate the appropriation of appraisals between the preparation and approval set
	

	```{r, fig.align='center', echo=FALSE, comment='', out.height = '40%'}
	# Review Training rating distribution
	edx %>% 
	  ggplot(aes(rating)) + 
	  geom_histogram(binwidth=0.2, color="darkblue", fill="lightblue") + 
	  ggtitle("Rating appropriation (Training)")
	```
	

	```{r, fig.align='center', echo=FALSE, comment='', out.height = '40%'}
	# Review Validation rating appropriation
	validation %>% 
	  ggplot(aes(rating)) + 
	  geom_histogram(binwidth=0.2, color="darkblue", fill="lightblue") +  
	  ggtitle("Rating Distribution (Validation)")
	```
	Both have very similar appropriation.
	

	\pagebreak
	We can plot the information and verify that a few films are evaluated more regularly than others.
	```{r, fig.align='center', echo=FALSE, comment='', out.height = '40%'}
	edx %>% 
	  count(movieId) %>% 
	  ggplot(aes(n)) + 
	  geom_histogram(bins = 30, binwidth=0.2, color="black", show.legend = FALSE, aes(fill = cut(n, 100))) + 
	  scale_x_log10() + 
	  ggtitle("Movies evaluated")
	```
	

	```{r, include=FALSE, echo=FALSE}
	# Extract release year from title into a separate field
	edx <- edx %>% mutate(releaseyear = as.numeric(str_extract(str_extract(title, "[/(]\\d{4}[/)]$"), regex("\\d{4}"))),title = str_remove(title, "[/(]\\d{4}[/)]$"))
	
	# Number of movies per year/decade
	movies_per_year <- edx %>%
	  select(movieId, releaseyear) %>% # select columns we need
	  group_by(releaseyear) %>% # group by year
	  summarise(count = n())  %>% # count movies per year
	  arrange(releaseyear)
	```
	

	We should survey what number of motion pictures were delivered throughout the most recent 80 years
	```{r, fig.align='center', echo=FALSE, comment='', out.height = '40%'}
	na.omit(movies_per_year) %>%
	  ggplot(aes(x = releaseyear, y = count)) +
	  geom_line(color="blue")
	```  
	We can see an exponential development of the film business and an unexpected drop in 2010. 


	
	The last is brought about by the way that the information is gathered until October 2009 so we don't have the full information for that period.
	

	

	```{r, include=FALSE, echo=FALSE}
	# What were the most popular movie genres year by year?
	genresByYear <- edx %>% 
	  separate_rows(genres, sep = "\\|") %>% 
	  select(movieId, releaseyear, genres) %>% 
	  group_by(releaseyear, genres) %>% 
	  summarise(count = n()) %>% arrange(desc(releaseyear))
	```
	\pagebreak
	We also note that different periods show certain genres being more popular during those periods. 
	```{r, fig.align='center', echo=FALSE, comment='', out.height = '40%'}
	ggplot(na.omit(genresByYear), aes(x = releaseyear, y = count)) + 
	  geom_col(aes(fill = genres), position = 'dodge') + 
	  theme_hc() + 
	  ylab('Number of Movies') + 
	  ggtitle('Popularity per year by Genre')
	```
	It will be very hard to incorporate genre into overall prediction given this fact.
	

	Let's review the number of times each user has reviewed movies. 
	```{r, fig.align='center', echo=FALSE, include=FALSE, comment=''}
	plt<-edx %>% count(userId) %>% 
	  ggplot(aes(n)) + 
	  geom_histogram(bins = 30, binwidth=0.2, color="black", show.legend = FALSE, aes(fill = cut(n, 100))) + 
	  scale_x_log10() + 
	  ggtitle("User Reviews")
	```
	```{r, fig.align='center', echo=FALSE, include=TRUE, comment='', out.height = '35%'}
	suppressWarnings(print(plt))
	```
	It seems most users have reviewed less than 200 movies.
	

	\pagebreak
	Finally let's plot the release year vs rating.
	```{r, fig.align='center', echo=FALSE, include=FALSE, comment=''}
	plt <- edx %>% group_by(releaseyear) %>%
	  summarize(rating = mean(rating)) %>%
	  ggplot(aes(releaseyear, rating)) +
	  geom_point(color='navy') +
	  theme_hc() + 
	  ggtitle("Release Year vs. Rating")
	```
	```{r, fig.align='center', echo=FALSE, include=TRUE, comment='', out.height = '40%'}
	suppressWarnings(print(plt))
	```
	Movies released prior to 1980 appear to get higher average ratings. This could allow us to penalize a movie based on release year by a calculated weight.
	

	```{r, echo=FALSE}
	rm(movies_per_year, genresByYear)
	```
	

	\pagebreak
	

	## Model Building and Training
	

	### Naive Models
	We start by writing a loss-function that computes the Residual Mean Squared Error ("typical error") as
	 our measure of accuracy. The value is the typical error in star rating we would make.
	```{r, echo=TRUE}
	RMSE <- function(true_ratings, predicted_ratings){
	  sqrt(mean((true_ratings - predicted_ratings)^2))
	}
	```
	

	We predict a new rating to be the average rating of all movies in our training dataset, which gives us a baseline RMSE. 
	We observe that the mean movie rating is a pretty generous > 3.5.
	```{r, echo=TRUE}
	mu <- mean(edx$rating)
	baseline_RMSE <- RMSE(edx$rating, mu)
	```
	

	We can now use this baseline model to predict against our test set and evaluate the resulting RMSE:
	```{r, echo=TRUE}
	naive_rmse <- RMSE(temp$rating, mu)
	naive_rmse
	```
	

	We know for a fact that a few motion pictures are simply commonly evaluated higher than others. We can utilize information to affirm this. For instance, if we think about motion pictures within excess of 1,000 appraisals, the SE mistake for the normal is all things considered 0.05. However, plotting these midpoints, we see a lot more prominent fluctuation than 0.05:
	
	Yet plotting these averages we see much greater variability than 0.05:
	```{r, fig.align='center', echo=FALSE, include=TRUE, comment='', out.height = '40%'}
	edx %>% group_by(movieId) %>% 
	  filter(n()>=1000) %>% 
	  summarize(avg_rating = mean(rating)) %>% 
	  qplot(avg_rating, geom = "histogram", color = I("black"), fill=I("navy"), bins=30, data = .)
	```
	

	\pagebreak
	So our intuition that different movies are rated differently is confirmed by data. We can augment our previous model by adding a term 
	to represent average ranking for a movie.
	```{r, echo=FALSE}
	movie_means <- edx %>% 
	  group_by(movieId) %>% 
	  summarize(b_i = mean(rating - mu))
	
	joined <- temp %>% 
	  left_join(movie_means, by='movieId')
	predicted_ratings <- mu + joined$b_i
	```
	

	Now we can form a prediction
	

	```{r, echo=TRUE}
	model2_rmse <- RMSE(predicted_ratings, temp$rating)
	model2_rmse
	```
	

	We already see a big improvement. Can we make it better? Let's explore where we made mistakes.
	

	```{r, echo=FALSE, include=FALSE}
	sliced <-temp %>% mutate(prediction = predicted_ratings, 
	                residual   = predicted_ratings - temp$rating) %>%
	  arrange(desc(abs(residual))) %>% 
	  left_join(movies) %>%  
	  select(title, prediction, residual)  
	```
	```{r, echo=FALSE, include=TRUE}
	sliced %>% slice(1:10)
	```
	

	These all seem like obscure movies. Many of them have large predictions. 
	\pagebreak
	

	

	Let's look at the top 10 worst and best movies.
	```{r, echo=FALSE, include=FALSE}
	movie_means <-  left_join(movie_means, movies) 
	```
	

	Here are the top ten movies:
	```{r, include=TRUE, echo=FALSE}
	arrange(movie_means, desc(b_i)) %>% 
	  mutate(prediction = mu + b_i) %>%
	  select(title, prediction) %>% 
	  slice(1:10)
	```
	

	

	Here are the bottom ten:
	

	```{r, include=TRUE, echo=FALSE}
	arrange(movie_means, b_i) %>% 
	  mutate(prediction = mu + b_i) %>%
	  select(title, prediction) %>% 
	  slice(1:10)
	```
	

	They all seem to be quite obscure. 
	

	\pagebreak
	

	Let's look at how often they are rated.
	```{r, include=FALSE, echo=FALSE}
	sliced<-edx %>%
	  count(movieId) %>%
	  left_join(movie_means) %>%
	  arrange(desc(b_i)) %>% 
	  mutate(prediction = mu + b_i) %>%
	  select(title, prediction, n) 
	```
	```{r, echo=FALSE, include=TRUE}
	sliced %>% slice(1:10)
	```
	

	```{r, include=FALSE, echo=FALSE}
	sliced <-edx %>%
	  count(movieId) %>%
	  left_join(movie_means) %>%
	  arrange(b_i) %>% 
	  mutate(prediction = mu + b_i) %>%
	  select(title, prediction, n)  
	```
	```{r, echo=FALSE, include=TRUE}
	sliced %>% slice(1:10)
	```
	
	

	So, the alleged "best" and "most exceedingly terrible" motion pictures were appraised by not very many clients. These motion pictures were for the most part dark ones. This is on the grounds that with only a couple of clients, we have more vulnerability. In this way, bigger evaluations of bi, negative, or positive, are more probable. These are "loud" gauges that we ought not trust, particularly with regards to forecast. Huge mistakes can build our RMSE, so we would prefer to be preservationist when not certain. 

	
	Regularization grants us to punish huge appraisals that originate from little example sizes. It has shared traits with the Bayesian methodology that "contracted" expectations. The general thought is to limit the whole of squares condition while punishing for enormous estimations of bi 


	

	\pagebreak
	Let's compute these regularized estimates of $b_i$ using lambda=5. Then, look at the top 10 best and terrible movies now.
	

	```{r, include=FALSE, echo=FALSE}
	lambda <- 5
	mu <- mean(edx$rating)
	movie_reg_means <- edx %>% 
	  group_by(movieId) %>% 
	  summarize(b_i = sum(rating - mu)/(n()+lambda), n_i = n()) %>%
	  left_join(movies) 
	
	# Top 10 best
	sliced <- edx %>%
	  count(movieId) %>%
	  left_join(movie_reg_means) %>%
	  arrange(desc(b_i)) %>% 
	  mutate(prediction = mu + b_i) %>%
	  select(title, prediction, n)
	```
	```{r, echo=FALSE, include=TRUE}
	sliced %>% slice(1:10)
	```
	

	```{r, include=FALSE, echo=FALSE}
	# Top ten worst
	edx %>%
	  count(movieId) %>%
	  left_join(movie_reg_means) %>%
	  arrange(b_i) %>% 
	  mutate(prediction = mu + b_i) %>%
	  select(title, prediction, n) 
	```
	```{r, echo=FALSE, include=TRUE}
	sliced %>% slice(1:10)
	```
	```{r, include=FALSE, echo=FALSE}
	# Do we improve our results?
	joined <- temp %>% 
	  left_join(movie_reg_means, by='movieId') %>% 
	  replace_na(list(b_i=0))
	
	predicted_ratings <- mu + joined$b_i
	model3_reg_rmse <- RMSE(predicted_ratings, temp$rating)
	```
	Did we improve our results?
	```{r, include=TRUE, echo=FALSE}
	model3_reg_rmse
	```
	

	We improved our results slightly. We can visualize how the predictions with a small $b_i$ are shrunken more towards 0.
	

	```{r, fig.align='center', echo=FALSE, include=TRUE, comment='', out.height = '30%'}
	data_frame(original = movie_means$b_i, 
	           regularlized = movie_reg_means$b_i, 
	           n = movie_reg_means$n_i) %>%
	  ggplot(aes(original, regularlized, size=log10(n))) + 
	  geom_point(shape=1, alpha=0.5)
	```
	

	\pagebreak
	We can try other values of lambda as well:
	

	```{r, include=FALSE, echo=FALSE}
	lambdas <- seq(0, 8, 0.25)
	mu <- mean(edx$rating)
	tmp <- edx %>% 
	  group_by(movieId) %>% 
	  summarize(sum = sum(rating - mu), n_i = n())
	
	rmses <- sapply(lambdas, function(l){
	  joined <- temp %>% 
	    left_join(tmp, by='movieId') %>% 
	    mutate(b_i = sum/(n_i+l)) %>%
	    replace_na(list(b_i=0))
	  predicted_ratings <- mu + joined$b_i
	  return(RMSE(predicted_ratings, temp$rating))
	})
	```
	```{r, fig.align='center', echo=FALSE, include=TRUE, comment='', out.height = '35%'}
	qplot(lambdas, rmses)
	```
	We find that lambda at 2.3 produces the most reduced RMSE. 

	
	We have improved the RMSE generously from our underlying guileless conjecture. What else would we be able to do to improve? We should figure the normal rating for client u, for those that have evaluated more than 100 motion pictures.
	
	

	```{r, fig.align='center', echo=FALSE, include=TRUE, comment='', out.height = '35%'}
	edx %>% 
	  group_by(userId) %>% 
	  summarize(b_u = mean(rating)) %>% 
	  filter(n() >= 100) %>%
	  ggplot(aes(b_u)) + 
	  geom_histogram(bins = 30, binwidth=0.2, color = I("black"), fill=I("navy"), show.legend = FALSE)
	```
	

	Note that there is considerable changeability across clients too. This implies a few clients are harsher than others and suggests that a further improvement can be made to our model. Presently it is conceivable that a few clients seem, by all accounts, to be harsher than others simply because they rate under-normal motion pictures. Thus, we like to appraise bu considering the bi. The least squares assessments will do this in any case, again we would prefer not to utilize lm here. 
	
	Rather we will take the normal of the residuals. We will utilize lambda = 2.3: 
	

	```{r, include=FALSE, echo=FALSE}
	lambda_2 <- 2.3
	user_reg_means <- edx %>% 
	  left_join(movie_reg_means) %>%
	  mutate(resids = rating - mu - b_i) %>% 
	  group_by(userId) %>%
	  summarize(b_u = sum(resids)/(n()+lambda_2))
	
	joined <- temp %>% 
	  left_join(movie_reg_means, by='movieId') %>% 
	  left_join(user_reg_means, by='userId') %>% 
	  replace_na(list(b_i=0, b_u=0))
	
	predicted_ratings <- mu + joined$b_i + joined$b_u
	model4_reg_rmse <- RMSE(predicted_ratings, temp$rating)
	```
	

	\pagebreak
	Note that the RMSE remains the same:
	```{r, include=TRUE, echo=FALSE}
	model4_reg_rmse
	```
	

	Let's measure the accuracy of our current model:
	

	```{r, include=FALSE, echo=FALSE}
	a <- predicted_ratings
	a <- ceiling(a / 0.5) * 0.5
	a[a <= 0.5] <- 0.5
	a[a >= 5] <- 5
	summarization = confusionMatrix(as.factor(a), as.factor(temp$rating))
	```
	```{r, include=TRUE, echo=FALSE}
	summarization$overall[1]
	```
	

	Our accuracy is currently at 22.6% which is much better than a coin-toss, but still far away from the 50% minimum required to score any points.
	

	### Different Procedures
	
	
	In the course of the most recent 3 weeks I have manufactured and tried various models utilizing various calculations, including Guileless Bayes, SVD, Shortened SVD, Framework Factorization, Neural Systems utilizing Keras on TensorFlow, Recommender lab and that's only the tip of the iceberg. Arbitrary Timberland on a little subset of 500,000 columns reliably scored above 80% precision, however because of asset imperatives and restrictions incorporated with different R libraries this could not be stretched out to the full preparing set of 9 million things. 
	
	Different equal libraries exist that can use machine bunches with numerous hubs to spread the preparation across machines, anyway for this undertaking crowd that would just not be feasible. All testing was performed on a Purplish blue occurrence with 20 vCPU's and 160GB Slam, with trade space stretching out that to over 220GB, yet this was completely devoured by calculations fit for taking care of the dataset, while others would expend around 90GB before running into their own inward confinements. One promising library (recosys) utilizes circle space as virtual Smash and figured out how to process the full set as a general rule, yet the precision score was no superior to the credulous model, much following 15 hours of boundary tuning with 10-overlay cross-approval
	
	

	### Incline One Model
	Incline One was presented by Daniel Lemire and Anna MacLachlan in their paper 'Slant One Indicators for Web based Rating-Based Synergistic Separating'. This calculation is perhaps the least difficult approaches to perform cooperative sifting dependent on things' closeness. This makes it exceptionally simple to execute and utilize, and precision of this calculation approaches the exactness of increasingly convoluted and asset concentrated calculations. 
	
	The Incline One strategy works with normal contrasts of appraisals between everything and makes expectations dependent on their weighted worth. 
	
	It is really quick, however for an enormous dataset it needs a ton of Slam. Consequently, we break the preparation set into 20 littler sets for preparing, and consolidation that into a solitary model toward the end. This procedure takes around 3 hours on a work area and requires in any event 32GB of Slam and 12GB of additional trade space. 
	
	Slant One was picked as it is a calculation that can bolster the parting of the preparation set into littler sets for handling, at that point re-consolidated preceding forecast without recognizable loss of exactness. 
	
	Parting was endeavored with Arbitrary Woods and it worked, anyway the exactness dipped under 40% when utilizing the combined model, so was disposed of as a choice. 
	
	The full Slant One source code is remembered for the going with MovieLensProject.R document and not executed as a component of this report because of asset imperatives. The yield of that is appeared in the disarray network underneath:





	

	```{r, out.width = "300px", fig.align='center', echo=FALSE}
	# get from https://github.com/gideonvos/MovieLens/blob/master/SlopeOneCM.png if you want to rerun this .rmd file
	knitr::include_graphics("SlopeOneCM.png") 
	```
	



	# 3. Results
	

	Precision for our gullible model topped at around 22%. Different understudies enhanced that by including type inclinations as a one-hot encoded vector and utilizing different strategies to bring extra information into the dataset, at that point applying Gullible Bayes for expectation, coming to as much as 60% in exactness. 
	Slant One, utilizing just 3 information components figured out how to score a precision pace of 85.14% on the approval set, with a RMSE of 0.192 which is a considerable improvement over the credulous model. In light of criticism from other Information Researchers utilizing Slant One on the MovieLens 10M dataset, this is actually true to form. 
	
	Exactness: 85.14% RMSE: 0.192 
	While it requires more Slam and took a few hours to prepare, the scoring of this test is on exactness, not speed of preparing or Smash imperatives of your machine. 
	We likewise abstained from expanding the dataset with extra realities.





	# 4. Conclusion
	As expressed before, I tried Innocent Bayes, Irregular Woodland, TensorFlow Neural Systems, PCA, SVD, Recom-menderlab, KNN, Kmeans, and different models and calculations. Some were quick yet the precision poor. Others were exact on littler sets (Arbitrary Backwoods) yet just could not scale to this informational index size and offered no dependable way to part and consolidate as I did with Slant One. While yet requiring a ton of Slam, Incline One was the most repeatable. I re-ran this model utilizing preparing subsets of 10 and 20 parts. The 10-split required 80GB of Smash while to 20 splits figured out how to fit into 32GB + 12GB trade space. An ordinary machine utilized for AI is frequently outfitted with more assets, particularly Slam and different GPU's, so our necessities are not over the standard.  At the point when tried, both the 10 and 20 split sets precisely scored the equivalent - 85%, hence demonstrating the parting and consolidating approach functions admirably on extremely huge datasets utilizing Slant One without exactness misfortune.
